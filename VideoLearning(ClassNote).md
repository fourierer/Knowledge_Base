# 课程笔记



#################################################################

# 黄凯奇《视频处理与分析》



### 第一章、绪论

#### 基本概念

1.视频就是利用人眼视觉暂留的原理，通过播放一系列图片，使人产生运动的感觉(实际上就是一系列图片)。

2.按照视频的处理和存储方式，视频分为模拟视频（Analog Video）和数字视频（Digital Video），数字视频是连续信号，模拟视频是采样和离散化的信号。以前的视频是模拟视频，现在的视频都是数字化的，本课程分析数字视频。

3.视频处理：视频增强、视频压缩等；视频分析：视频检索、视频监控等。

4.Frame rate：帧率

Fps：frame per second，现在认为24帧每秒很流畅

5.视频增强为了改善视频的质量；

视频压缩为了传输和存储，比如现在的视频直播，压缩技术使得过程非常流畅；

视频有冗余需要去除，存储设备会耗费大量财力物力。视频处理是不够的，海量的视频数据需要去分析解决。

视频分析关键技术：运动目标检测、运动目标跟踪（在哪？），目标分类识别（是谁？），异常行为分析（在干什么？）。



#### 视频技术的应用：

1.安全领域

家庭安全：智能家居，如独居老人在家摔倒，智能家居可以自动识别；

交通安全：辅助驾驶监控，防止危险发生；

公共安全：监控

个人安全：生物特征识别（虹膜识别），考勤通关等

2.娱乐领域

笑脸检测，人脸美颜（直播领域较多），电影特效，风格迁移，人机交互（一般对光照要求较高）

3.生活便捷

增强现实（AR），街景（图像拼接，3D建模）（VR），字符识别（OCR）（车牌识别），物体识别（无人超市，但是没有继续做下去，存在很多技术问题），Google glass，

4.其他领域

导购机器人，物流机器人，医学





### 第二章、视频分析系统及应用

#### 一、视频监控技术发展

1.监控技术发展的三个阶段

人力现场监控（人眼+人脑）--传统视频监控（机器眼+人脑）--智能视觉监控（机器眼+机器脑）

2.视频系统架构

（1）摄像机：视频传感器（CCD、CMOS）+处理器

（2）传输：电缆、光纤、网络传输、微波传输

（3）显示：

3.智能视频监控核心技术

微处理器（模式视频监控系统）、数字压缩编码技术（数字视频监控系统）、网络技术，高清技术（网络监控，高清监控）、计算机视觉技术和模式识别技术等。

（1）运动目标检测：

静态图片中的物体检测研究：

2012年以前，都在voc数据集（20个类别）上测评，而且用的都是比较传统的方法；

2012年开始在ImageNet上测评和比赛，开始使用deep learning的方法；

现在基本上使用微软的MS COCO数据集上测评；

主要算法有R-CNN,Fast-RCNN,Faster-RCNN等



动态图片（视频）中的物体检测研究：

UCF-50



（2）运动目标跟踪（在单帧目标检测的基础上加上多帧检测目标id不变）

单相机单目标跟踪、单相机多目标跟踪、多相机单目标跟踪、多相机多目标跟踪等。

难点：人密集的场合、球场上人的衣服非常相似等场合。





（3）目标分类和识别：

物体分类：AlexNet(2012),VGG(2014),GoogleNet(2014),ResNet(2015),DenceNet(2016),2019年已经取消了比赛，因为这个数据集已经起不到推动物体分类算法发展的作用了。

行人重识别（Person Re-identity）:

类似于目标跟踪，只不过是两张图片的matching,而不是目标跟踪的动态跟踪。

数据集：RAP(Richly Annotated Pedestrian) Dataset,室内属性



（4）异常行为分析：

异常行为是（行为识别，视频分类）的一个特例，要分辨出异常行为所在的那一类。

数据集：
UCF,HMDB,Kinetics,something，这些数据集类别的定义实际上在一定程度上会限制行为识别的发展。

#### 二、智能视频监控技术应用

1.检测类：攀爬检测、禁区检测

2.检测+分类：非法停车、丢包检测

3.检测+跟踪：徘徊检测，自动跟踪，人数统计

4.目标分类，行为分析：车牌识别，打架检测

5.其他：火警检测



发展阶段：

1.2004-2007:起步阶段，主要功能为入侵检测，行业集中在轨道交通；

2.2008-2009:人群密度检测，徘徊检测；

3.2010-：接力跟踪；

需求还在扩大，由单一应用转向多样。



### 第三章、目标物体识别

#### 一、总体介绍和发展历史

1.总体介绍：物体识别包括领域

物体分类：识别是否为飞机

物体检测：用bounding box将飞机框出来

物体分割：将飞机与背景图分割出来



2.发展历史

（1）1965-1980:配准（Alignment）

需要在图中找出目标物体，特征点匹配（刚体配准相对简单），通过基元（geon）来匹配识别。

（2）Eigenface（PCA）

（3）sliding windows

（4）1990-？local feature，基于特征

（5）2000-？Parts and Structure

（6）2003-present：bags of features models

（7）global feature

（8）2011-present：Deep Learning



物体识别任务中，背景对物体的影响：

1.如果识别的物体较简单，和背景差别较大，此时可以不需要背景；当要识别的物体有遮挡，很难直接识别，此时需要背景的辅助来推理；

2.attention机制实际上就是加入推理内容，比如加强背景和目标之间的关系（Non-local），视频分类中帧与帧之间的关系；



#### 二、典型方向和研究方法

**典型方向：**

1.RGB-D图像识别（RGBD image recognition）

和普通RGB图像相比，多了个D



2.视频分类/识别（Video Classification ）

和Image classification不一样的是，视频分类中最重要的是如何建模时序信息。



3.行人再识别（Fine-Gained image recognition）

行人重识别，相当于对同一个人进行识别，可以利用不同的属性进行细粒度识别（即识别一个具体类中，精细的小类，需要关注更细节的信息）





**研究方法：**

**1.词包模型（bags of feature）**

Image classification:

（1）特征提取：比如将图像分为很多不同的patch

（2）学习“视觉词典”：

一般是采用聚类方法，形成词典，又称为码本（code book）

（3）使用“视觉词典”来量化特征（coding）

Sparse coding SPM（一个特征和多个基相关），LLC（Locality-constrained Linear Coding，认为基与基之间有相关性）

（4）池化，获得更稳定的特征，最大汇聚和平均汇聚



**2.深度学习**

ImageNet和深度卷积神经网络相互成就。

1.1998年：LeCun提出LeNet，用来字符识别；

2.2012年：Alex提出AlexNet，用于ImageNet识别；（1.改进了优化方法，随机梯度下降方法的应用；2.使用非监督数据来预训练；3.使用了GPU加速）；

3.VGG，相比于AlexNet，使用小卷积核核小池化核，层数更深；

4.GoogleNet，在网络需要的计算不变的前提下，通过工艺改进提升网络的宽度和深度，即在增加网络深度和宽度的基础上，控制参数数量和计算量；

5.ResNet，解决了深度网络中无法有效传播梯度的关系，重新定向了深层神经网络中的信息流；

6.DenceNet，Xception以及一系列轻量级网络ShuffleNet，MobileNet(V1,V2,V3)，GhostNet(2020CVPR)；









### 第四章、视频编解码与目标跟踪

#### 一、视频编解码

1.视频压缩可以进行的原因

（1）视频中有很多冗余：空间冗余（空间中目标区域只有一小部分）、时间冗余（连续帧很相似）、结构冗余（图像中的像素存在明显的分布模式）、信息熵冗余、知识冗余（由很多先验知识可以得到）、视觉冗余（人的视觉系统的分辨能力为26灰度等级，多的级别无法察觉）

（2）人的视觉特征表现为对亮度信息很敏感，而对边缘的急剧变化不敏感



2.数据压缩算法的综合评价指标

（1）压缩倍数（压缩率）：由压缩前后的数据量之比或者压缩后的比特流中每个显示像素的平均比特数来表示

（2）图像质量：主观评价和客观评价（信噪比）

（3）压缩和解压缩的速度：对称压缩（要求解压缩和压缩都需要实时进行）、非对称压缩（只要求解压缩是实时的）、压缩的计算量（一般编码的计算量要比解码的计算量大）



3.数据压缩算法分类

（1）按照压缩方法是否产生失真分类：无损压缩（RLE/JPEG/MPEG/H264，一般无损压缩饿压缩率只有3倍左右，无法满足需求），有损压缩（主要是去除时空冗余，再用变换将信息集中到少数几个部分，主要保存模型参数和变换方法。方法有预测编码、变换编码（DCT变换，小波变换，KLT变换））

有损的$8\times8$DCT主要应用于JPEG，H.261，H.263等

无损的$4\times4/8\times8$整数DCT以及哈达码变换应用于H.264

小波变换被应用于JPEG2000以及H.265中



4.视频压缩的几个重要压缩标准

ISO的MPEG-2,MPEG-4、ITU的H.264、中国的AVS

（1）很多主流媒体采用MPEG-4压缩标准：如avi,mov,asf,mp4

（2）H.264只是MPEG-4的一部分

（3）H.263和AVS编码效率高于MPEG-2



#### 二、视觉目标跟踪

1.简单介绍

（1）概念：在连续的图像帧中跟踪指定目标，比较精确地估计出该目标在每一帧图像中的位置

（2）挑战：非刚体、光线变化、跟踪目标所占区域小、跟踪数目太多、再识别、实时性

（3）应用：基于运动的识别、人机交互



2.单相机单目标跟踪

（1）简介

特点：时间空间都连续

优点：对于固定相机，算法简单，计算量小

缺点：视野较小，遮挡严重

（2）数据库

TB-100，VOT，TrackingNet，LaSOT，GOT-10k(one-shot，训练和测试的类别不一样，比如测试集中有狗的跟踪，但是训练集中并没有狗的跟踪类别)

（3）算法分类

1.目标表达表达算法（形状表达、表观表达）

主要算法：根据目标区域的统计特性来区分背景和目标所在区域

2.时序预测模型

主要算法：

（1）Kalman滤波

（2）粒子滤波：准确率高，计算量大

（3）Mean Shift：计算量小，但准确率不如粒子滤波

（4）轮廓跟踪：不适用于轮廓变化非常大的跟踪

3.基于分类（检测）的跟踪：区分目标和背景的判别式模型

（1）TLD

（2）Struck

（3）CSK

4.基于深度学习的跟踪方法（对外观变化和物体旋转会更具有鲁棒性）

（1）SiamRPN：使用RPN

（2）SiamMask



3.单相机多目标跟踪

（1）简介：单相机多目标跟踪是对场景中的每一个目标都进行跟踪，相比于单相机多目标跟踪，会出现目标之间相互影响（如目标之间遮挡、轨迹交叉等），目标之间很相似（如衣服相似）。

（2）跟踪方法分类：点跟踪（将目标处理成点，不考虑表观，适用于大视野中的小目标），核跟踪。

难点：数据关联，单相机中多个目标之间会有关联。

（3）数据集：PETS2009，KITTI-Tracking，MOT dataset，UA-DETRAC，DuckMTMC（也是多相机多目标跟踪的数据集）；

竞赛：MOT challenge，AVSS challenge

（4）具体的单相机多目标跟踪方法

1）CEM（Continuous Energy Minimization，CVPR2011、PAMI2014）

优点：采集到所有帧的检测样本之后统一关联，可以建立长的完整轨迹；

缺点：计算量大，难以实时，表观模型难以更新，只能离线检测；

下面的都是在线逐帧的检测方法。

2）

3）AMIR（ICCV2017）

4）RNN_LSTM



4.多相机目标跟踪

简介：目前的监控技术，多个摄像机协作，构成一个多摄像机系统，可以增大监控范围，提高资源利用率；

和单相机相比，单相机主要是要建立连续帧相同目标的对应关系（时间上数据关联），多相机还要建立不同摄像机同时或者间隔一段时间的相同目标之间的对应关系；（时间上和空间上数据关联）



4.1.重叠场景摄像机网络

（1）简介：相邻的摄像机视野之间有重叠，可以作为先验知识；

（2）单相机下可以通过对表观，轮廓进行建模预测；但是长时间被遮挡，或者完全被遮挡，这些方法就不work；

（3）具体算法

1）Foreground Likelihood，不依赖于相机标定

2）

3）Principal axis

4）Hypergraphs，超图（CVPR2013）



4.2.非重叠场景摄像机网络（MCT）

（1）简介：摄像机之间会有盲区，盲区的时间也不确定，同一目标相同时间不会出现在不同摄像机上。如果一个目标进入视野，就需要重新识别他是“之前离开后再次进入的”还是“新目标”。如果是前者，需要识别他的身份，保持身份不变；如果是后者，则建立新的身份。

（2）跟re-id区别：从任务目标来看很相似，实际上不一样。MCT问题是一个分类问题，给定$n$个相机流，决定目标在哪里；re-id是一个排序问题，对于给定一个查询图像，需要从数据库中找出最相似的那个那个id（当然也有把re-id当成分类任务来做）。

（3）数据库：PETS2009，DuckMTMC，EPFL，CAMPUS，MOCAT

（4）多摄像机拓扑结构的建模，可以两两摄像机进行建模

（5）具体方法：

1）

2）多摄像机之间光照变化的处理（消除光照的影响）：



目标跟踪总结和展望：

总结：

（1）算法性能受底层算法影响严重；

（2）算法的跟踪准确率与运行效率很难同时兼顾；（算法的实时性和准确率无法同时兼容）

（3）多摄像机目标跟踪通用性差；（要先验获取摄像机网络的拓扑结构，算法受限于特定的摄像机网络）

展望：

复杂场景、大规模、高效高性能。



### 第五章、物体检测

#### 1.任务基础

1.定义：

检测出图像中不同物体的位置（多物体的分类和定位）；

定位和检测的区别：定位只需定位某一个物体的位置，检测需要检测出所有不同物体的位置；

物体识别：识别任务一般涵盖分类和检测；



2.物体检测分类

按检测的目标划分：

（1）通用物体检测：检测出所有的物体；

（2）特例物体检测：对特定物体的不同个体进行检测，比如行人检测，车辆检测，交通标识检测，人脸检测，头部检测



按数据的模态划分：

自然图像/视频检测，RGB-D物体检测（color and depth image pair，RGB彩色图像+深度（depth）图像），3D物体检测



按照拍摄的视角划分：

监控场景，航拍/无人机，卫星图像



按训练的方式（监督信息）划分：

全监督训练方式（所有类别有类别，所有类别有定位），半监督训练方式（所有类别有类别），弱监督训练方式（所有类别有分类，所有类别没定位），混合监督训练方式（所有类别有类别，部分类别有定位）



3.物体检测挑战

（1）光照、视角、姿态、遮挡等；

（2）类内差大，类内差小；



4.物体检测数据集

PASCAL VOC，ILSVRC，MS-COCO，OICOD(目前最大规模的物体检测数据库)

专用数据集不太适合用于预训练，多样性高的数据集比较适合用来预训练模型。



5.物体检测评估准则

（1）IoU，交并比

（2）精度，召回率：两个呈负相关

（3）mAP：mean Average Precision，多类的检测指标



#### 2.研究发展与代表性方法

1.传统方法

（1）VJ Detector：使用Haar特征以及Adaboost来检测人脸

（2）HOG Detector：

（3）DPM ：可形变部件模型



2.基于深度学习的方法

两阶段检测模型：对场景进行粗略扫描，将注意力聚焦在RoIs中，检测模型提取该区域的特征。

（1）RCNN：需要在大规模分类数据集上进行预训练，然后在检测数据集上进行fine-tune；

具体步骤：

对输入图像给出候选兴趣区域，对候选区域进行变换（warp region）输入到卷积网络中，对感兴趣区域进行微调，微调的类别是n+1类，n个类别+背景类；除此之外还有SVM进行位置回归的训练，整个过程很耗时间。

（2）SPP-Net：省略RCNN中的两千多个候选框，直接将图像输入一个卷积网络提特征，直接获得候选区域的feature map，后续和RCNN是一样的。

（3）Fast-RCNN：规避了冗余的候选框过程，和SPP-Net一样一次性提取候选框特征。

这些特征一方面进行softmax分类，一部分进行位置回归，将两部分的损失相加（分类损失和回归损失），可以视为端到端的训练。

（4）Faster-RCNN：使用网络训练的方式生成候选区域，而不是使用之前手动的2000多个候选框。使用RPN生成候选框，输入检测部分。检测部分和Fast-RCNN是一样的。

RPN：使用锚点（anchor），类似于滑动窗口的方法找到候选区域，最终只需300多个候选框，比2000多个候选框少了个数量级。

（5）Mask-RCNN：改进了RoI pooling



单阶段检测模型：

（1）YOLO：you only look once，将检测问题转换为回归问题，网络模型直接回归框的位置，进行端到端训练。优势是速度很快。

流程：

1）首先将图像resize，将图像等分为$s*s$的grid，确定每个bounding box的中心在哪个grid里面；

2）每个网格会预测$B$个bounding box和$C$个类别标签，每个bounding box 有中心坐标$(x,y)$和长宽$(w,h)$四个自由度，加一个置信度得分，所以每个网络有$5B+C$个输出；

3）预测每个bounding box的得分，是某类的概率，条件概率。



单阶段方法的精度目前还比不上两阶段的方法，但是在速度上有优势。



#### 3.不同监督信息下的物体检测

类别标签很好获得，但是位置标签很难标注。



#### 4.视频物体检测

视频中经常会出现遮挡，运动较快，以及姿态变化等情况。

总结：

（1）视频物体检测目前仍处于持续探索的阶段；

（2）视频物体检测一般离不开强大的图像检测算法，通常采用策略是：图像检测器+时序信息融合；

（3）视频中有大量冗余信息，除了提升物体检测算法性能，减少冗余信息的提取和计算，提高效率也是值得关注的问题。





### 第六章、目标分割

#### 1.物体分割

1.物体分割分类：二值分割（边缘检测，阴影检测与去除，显著性物体检测），语义分割（按照语义将图片分割几个区域，按照语义打标签），实例分割（相同语义的实例也需要分割出来，打上实例标签），全景分割（结合语义分割和实例分割，需要同时分割出语义，以及语义中的实例，难度最高）

2.物体分割应用：人体解析，服装解析，基于深度图像（RGB-D）的场景语义理解，多任务分割，前景分割，图像着色，视频分割，点云场景理解，3D场景理解

3.物体分割历史方法

（1）阈值法

（2）基于聚类的方法

1）均值聚类：将像素分为$k$个类别

2）超像素：预先定义超像素的个数，再使用聚类方法，生成超像素

（3）基于图的方法：ps软件上经常会有抠图操作，在特定区域划一条线，自动分割出区域

（4）CNN：

1）FCN(fully convolutional network，skip connection)：是个端对端的网络，直接输出一个分割好的图像，局限性：没有考虑像素之间的关系，结果不够精细

2）Encoder-Decoder Variants：提出卷积-池化层，以及反卷积-反池化层

3）U-Net：和Encoder-Decoder非常相似，不一样的是在Decoder的过程融合了很多Encoder过程的信息。

4）DeepLabV1：CRF，V2：CRF+ASPP，V3：ASPP Variants





#### 2.语义分割







#### 3.实例分割

1.数据集：

（1）ImageNet(类别多，但物体都位于正中间)；

（2）PASCAl VOC(类别少，只有20个类别)；

（3）SUN；

（4）MS COCO(91类，三万多幅图像，包含单个物体实例标注比较多，不像ImageNet那样，MS COCO一副图中可能有很多类别的物体)。MS COCO数据集中存在一些问题，如

1）标注不明，比如人和背包标注为一类；

2）未达到实例级标注，比如多人合影，并没有标注出每个人；

（5）Cityscapes数据集

自动驾驶数据集，包含了五十多个城市的街景，主要关心交通中的各个物体

（6）ADE20K-Scene Parsing Dataset，150类，2万张图像



2.代表性方法

（1）早期方法：DeepMask，ISFCN

（2）Top-down methods and bottom-up methods

Top-down methods：基于检测的方法(检测是定位物体的位置，实例分割需要给出出每个像素点的label)；

这类方法基本都是在物体检测的bounding box之上增加mask prediction模块，检测时即实现不同不同instance的区分，在检测框内进一步区分前景和背景，这类方法往往可以取得好的效果；

主要方法有：SDS，MNCs，FCIS，Mask R-CNN，MaskLab，PANet；

Bottom-up methods：基于分割的方法（基于聚类的方法，Clustering based methods）

首先进行像素级别的语义分割，再进一步区分不同的instance（即先区分像素的类别，再区分同一类别里面具体是哪个instance）；



（3）Single shot instance segmentation（单阶段实例分割）

主要方法有：TensorMask，PolarMask，SOLO（联系YOLO）

思路受one-stage detector启发，非常有启发性，轮廓较为粗糙，性能不如其他方法，但相比于bottom-up模型（基于分割的模型）和top-down模型（基于检测的模型），其计算框架大大简化。



#### 4.全景分割

深度学习在计算机视觉发展领域：

classification(2012年AlexNet)

detection(2015年Faster R-CNN)

instance segmentation(2017年Mask R-CNN)

Panoptic Segmentation(2019年)

代表性方法：

（1）Mask R-CNN+PSPNet Combination Heuristic

（2）Obiect Context Network

（3）Panoptic FPN（2019年cvpr）

（4）UPSNet

（5）Panoptic DeepLab

最直接的思路就是在semantic segmentation和instance segmentation的基础上添加后处理模块，实现panoptic segmentation



### 第七章、视频行为识别

#### 一、行为识别

1.研究目标

行为识别算法要求对发生在视觉场景中的目标运动行为进行语义理解和语言描述，是计算机视觉研究的终极目标，即读懂视频中的行为类型。

行为体现在动作本身的样子以及时序信息中。



2.行为分类

（1）Gesture：人体某一部分的运动，如“挥手”；

（2）Action：单人的简单运动，如“行走”；

（3）Interaction：人与人或人与物的交互行为，如“握手”，“喝水”；

（4）Group Activity：物理或概念上的人群行为，如“开会”；

**简单行为（Action）的识别称为行为识别，复杂行为（Activaty）的识别称为事件识别，一般行为识别都是指简单行为的识别。**



3.行为识别常用特征（早期行为识别方法）

（1）方向梯度直方图HOG；

（2）光流直方图：HOF；

（3）光流梯度直方图MBH；

（4）轨迹特征Trajectories；

（5）人体骨骼特征；



4.数据库介绍

（1）早期研究

1）KTH数据库：6中单人动作，599段视频；

2）Weizmann Dataset：10中单人行为，共90段视频；

这两个数据库测试准确率已经达到100%。

3）IXMAS数据库：多视角行为数据库，13种单人行为，共165段视频；



（2）多媒体数据库

1）Hollywood数据库（2008）:8种行为，475段视频来自32部好莱坞电影；

2）HMDB51数据库（2011，布朗大学）：51类行为，6849段视频，大部分来自电影片段，其余来自网络视频网站；

3）UCF-50&UCF-101（2012）：从YouTube上摘取下来；

4）Olympic\UCF Sports



（3）监控数据库

1）PETS数据库：9个视角，用于目标跟踪；

2）CASIA行为分析数据库：3个视角，6种单人行为，7种交互行为，共1446段视频；

3）VIRAT数据库；



（4）当深度学习方法流行之后，需要建立更大的行为识别数据库：

1）Sports-1M Dataset（2014）：从Youtube上取得，487种运动类别标签，1133159个视频片段；

2）FCVID（2015）：不仅仅局限于运动类别，还有社交等方面，共91223个样本，239类；

3）Columbia EventNet（2015）：95K YouTube视频样本，500类事件；

4）ActivityNet Dataset（2015）：Activity-100，Activity-200；

5）Youtube-8M（2016）：4716个动作类别，8000000个视频片段，2018年移除了部分低质量视频；

6）Kinetics-400&Kinetics-600（2017）：2017年400个类别，30多万视频；2018年600个类别，50万左右的视频，来源于Youtube，每个动作都有400-1150个视频片段，每段视频的时长都在10秒左右；

**7）Moments in Time Dataset（2017）：共有1000000个视频，每个视频的长度均为3s，动作主体可以是人，动物，物体乃至自然现象（先确定了标签，再根据标创建动作数据样本），数据集的类内差异和类间差异均很大；**

8）AVA Actions Dataset：80类原子动作类别，210000种动作类别，57600个视频片段；

9）Charades（2017）：

10）Charades-Edge（2018）：同时提供了第一视角和第三视角的视频；



5.算法介绍

5.1.人体动作识别

（1）基于模版匹配的动作识别



（2）基于局部特征的动作识别

DT，IDT（深度学习之前的最好方法之一）



5.2.复杂事件分析

（1）复杂行为分析：包含一个或多个行为主体，可表示为一系列子行为依据一定的时空逻辑关系组合而成。需要先识别出原子行为，在建模出原子行为之间的关系；

（2）



5.3群体行为分析

（1）Two-stream Convolution Network

一个通道提取静态图片序列的表观特征；

另一个通道输入图片序列的运动图提取场景动态信息；

（2）three-branch Slicing CNN（S-CNN）

群体行为分析发展轨迹：从低密度群体到高密度群体；从底层图像特征、提取视频序列中的群体的运动轨迹到高层结构建模，网络自我学习；从短时运动分析到长时运动分析；



6.拓展研究之Action Detection

不仅要识别类别，还要检测出位置



#### 二、异常行为检测

1.异常行为特点

（1）稀疏，发生频率低；

（2）依赖于对“正常”定义的尺度；

（3）非常依赖于场景，同一种行为在不同场景下可能是异常行为，也可能不是。

2.与行为识别的区别

（1）大多数时候并不需要检测出具体的行为类别；

（2）异常行为种类非常多，类内行为差距很大；

（3）正负样本不均衡，一般正样本<<负样本

3.数据集

无监督学习数据集：

（1）UCSD Anomaly Detection Dataset

（2）ShanghaiTech Campus Dataset

弱监督学习数据集：

UCF Crime Dataset

4.方法

（1）无监督异常行为检测方法

（2）弱监督异常行为检测方法

5.总结

目前的异常行为检测还没有理解场景的能力，并且数据库的规模和多样性远不如行为识别的数据库



#### 三、视频描述（Video Captioning）

1.定义：对视频自动生成一句话来描述

2.难点：需要识别出时序信息和声音信息

3.数据集

（1）Youtube2Text：1970个短视频，每个视频片段大概在10-25s之间，通常只包含单个活动；训练集含有1200个，验证集含有100个以及测试集含有670个视频片段；平均每个视频有40多个描述；

（2）MSR-VTT Dataset：包含10000个训练视频片段，总长度41.2小时；每个片段大约有20个自然语言描述语句，共200000个语句；数据集中还提供了音频信息；

（3）VATEX Dataset：数据集包含41300个视频和82.6万中英文视频描述，数据集共涵盖600个人类活动；

4.评价准则

（1）BLEU：基于准确率的相似性度量指标，包括BLEU-1，BLEU-2，BLEU-3，BLEU-4，主要思想是基于人工标注语句与生成语句之间n个连续字符的严格匹配情况进行评价；指标越大，场景描述越好；

（2）ROUGE：包括ROUGE-L，ROUGE-N；ROUGE-N基于召回率的相似性度量指标，ROUGE-L基于最长公共子序列；

（3）CIDEr：同时考虑了准确度和召回率；

（4）METEOR：word to word的匹配

5.方法（主要是深度学习方法）

根据编解码阶段用的方法的不同，分为CNN-RNN，RNN-RNN，基于强化学习的视频语义描述方法三个大类。

（1）CNN-RNN

1）LSTM-YT：对视频逐帧卷积进行编码，再利用RNN进行解码，缺点是没有用到视频的时序信息；

2）TA(Temporal Attention)：将视频的时序信息考虑进来

3）GRU-EVE：

（2）RNN-RNN

1）Composite LSTM：并不是做captioning的，实际上是做行为识别的文章；

2）H-RNN：对视频生成了多句话；

（3）基于强化学习

总体也遵循编码-解码的结构。

1）HRL模型：

（4）方法总结：

CNN-RNN/RNN-RNN：优点是采用监督学习的训练方法，有直接的成本函数。缺点是基于监督学习的方式进行训练，需要大量的label训练数据，训练损失和评价标准不统一；

强化学习：优点是模型训练和评估时的情形一致，模型训练时直接最大化评价指标CIDER等。缺点是学习机制很难设计。



#### 三、视频问答（Video Question Answering, VQA）

1.数据集：MovieQA，TGIF-QA，MSVD-QA，TVQA；

2.方法：TVQA模型，LMN模型，





#################################################################

## 胡占义《计算机视觉》

### 第一章、绪论

#### 一、Marr计算机视觉简介

1.计算机视觉奠基人：David Marr

2.对于计算机视觉，输入是图像或者视频，输出根据任务确定。

3.当前人工智能两大主要途径：（1）大数据+深度学习；（2）通过模拟大脑：系统复杂到某个地步，很难从理论上解析。

4.深度学习里程碑式的研究：（1）1958年，Perceptron，单层神经网络无法实现异或运算；（2）1975年，BP算法；（3）2006年，Hinton以及2012年AlexNet；

5.在**物体视觉**方面，如物体识别和分类等，目前深度学习方法超过了其他所有方法，甚至超过了人类视觉系统；但是在**空间视觉**方面，如视觉定位，三维物体重建，深度学习方法仍无法与基于几何的方法相媲美。



#### 二、计算机视觉发展历史与现状

主要四大阶段。

三大会议：ICCV,ECCV(便理论),CVPR(偏理论)

两大刊物：PAMI,IJCV



1.马尔计算视觉理论（1981-）

马尔认为：类似于计算机对给定为题的计算过程，视觉感知是一个“对图像信息的逐层加工处理过程”。

马尔认为视觉的主要目标是重建物体的三维形状，从现在的生物视觉的研究进展看这是不正确的。物体识别是基于二维图像的，深度信息起的作用不大，即物体识别不必先进行三维形状重建。



2.主动视觉大辩论（1988-1994）

马尔的三维重建理论缺乏目的性以及与环境的主动交互性（主动视觉）



3.分层三维重建理论（1992-）

计算机视觉从工业应用到精度要求不太高的领域，如通讯，虚拟现实，考古，是计算机视觉第二次发展高潮的主要原因。

图像--射影空间--放射空间--欧式空间



4.基于学习的视觉（2001）

（1）子空间方法，如流形学习；

（2）深度学习；



图像物体识别代表性理论：马尔三维重建理论、巴乔的二维图像模型、低卡洛的分层去纠缠理论物体识别的逆生成模型





### 第二章、计算机视觉

#### 1.深度学习与卷积神经网络



#### 2.图像底层特征提取

特征提取：

能有效反映图像内容的信息就是特征，对于图像，边缘和轮廓能反映图像内容。

1.边缘提取

（1）边缘定义：

边缘是图像中亮度突然变化的区域，图像灰度构成的曲面上的陡峭区域，像素灰度存在阶梯状或屋脊状变化的区域。

（2）图像微分算子：

一维：一阶导数的极值点，二阶导数的过零点；

梯度向量：$\nabla I(x,y)=(\frac{\partial I}{\partial x},\frac{\partial I}{\partial y})^T$，每一个像素都可以计算出梯度向量

二维：拉普拉斯算子$\triangle I=\nabla^2I=\frac{\partial^2I}{\partial x^2}+\frac{\partial^2I}{\partial y^2}$

水平梯度算子（检测竖直边缘），竖直梯度算子（检测水平边缘）

1）拉普拉斯算子：只用一个模版便可计算得到，实际中几乎不独立使用拉普拉斯算子，二次求导数对噪声非常敏感，配合卷积模糊使用。

2）高斯拉普拉斯算子（LoG）：先用高斯核进行卷积模糊，再利用拉普拉斯算子检测边缘，等价于先对高斯核求拉普拉斯，再用拉普拉斯卷积核整体对图像做卷积。

（3）canny算子

1）首先计算图像与高斯核的卷积，即做模糊；

2）用一阶有限差分计算偏导数的两个阵列，即计算两个方向的梯度，此时每一个像素都有一个梯度向量；

3）计算每个像素的梯度向量的方向和辐值，非极大值抑制，在该点周围方向（8个方向）选取多个像素点再计算多个辐值，如果该像素的梯度向量的辐值是极大值，则作为候选边缘点；

4）采用双阈值方法，低阈值得到低阈值边缘图，高阈值得到高阈值边缘图，认为高阈值边缘点是可靠的，低阈值边缘点可以用于修正边缘，如果某低阈值边缘能够连接到高阈值边缘，则认为该低阈值边缘点可靠；



2.特征点提取

近几年的边缘检测的文章很少了，特征点还是有很多。

2.1.角点检测：

（1）Harris角点检测算法：

角点定义：以某个点为中心做窗口，窗口向**任意方向**的移动都导致窗口中图像灰度的明显变化，则该点称为角点。

（2）将窗口平移$[u,v]$，产生的灰度变化$E(u,v)$
$$
E(u,v)=\sum_{x,y}w(x,y)[I(x+u,y+v)-I(x,y)]^2
$$
对$M$的特征值进行分类：

1）如果两个特征值都比较大，则是角点；

2）如果一个特征值较大，一个特征值较小，则是边缘；

3）如果两个特征值都比较小，则是平坦区域；

引入响应函数$R$，来判断是否为角点。



（3）Harris角点性质：

1）旋转不变性，由于旋转之后仍然在该方向有变化，所以角点仍能被检测；

2）部分的仿射不变性（加性的放射不变性），对图像灰度做加减运算，仍能检测出角点；

3）不具有尺度不变性，比如图像缩小时，用于检测的窗口中灰度变化差与之前的灰度变化差不一样，判断结果会发生变化；



2.2.ORB特征

1.FAST：判断该点的灰度是否为周边的极大值，如果是则判断为特征点；

2.BRIEF描述

汉明距离（Hamming Distance）：两个字符串的之间的汉明距离是指两个字符串对应位置的不同字符的个数



3.SIFT（Scale Invariant Feature Transform）特征点检测

（1）SIFT是一种特征提取算法，DoG特征检测+SIFT描述子，是最佳的描述子，没有之一。

（2）不变性：旋转不变性，尺度不变性

（3）SIFT特征提取流程

1）图像的尺度空间

用不同的高斯卷积核与原图像做卷积，不同的核尺寸$\sigma$的卷积结果构成图像尺度空间；

2）高斯差分尺度空间

图像的尺度空间中的图作差，DoG和LoG比较相似；

3）高斯金字塔

用不同尺寸的高斯卷积核对原图做卷积，相邻尺寸的卷积结果相减；将图像缩放（下采样）再进行相同步骤；得到高斯金字塔。

4）DoG尺度空间极值点检测

将高斯金字塔建立三维尺度空间，搜索每个点的$2\sigma$领域（三维空间领域），若该点为局部极值点，则保存为候选关键点；

5）关键点的精确定位

在离散采样中搜索到的极值点不一定是真实空间的极值点，用一个函数去拟合DoG，计算真正的极值点（如何定位真正的极值点？真正的极值点不一定是整像素）；

6）去除不稳点的关键点

i）去除对比度低的点，设定阈值

i）去除边缘上的点

（4）特征点的描述

如何构造一个有区分性的特征点描述向量？

1）以特征点为中心，确定一个圆形邻域，邻域中的每个点都有方向和辐值。将方向分为多个区间（假设为8个，区间边界为方向），根据方向将这些点划分到各个区间里面，每个点对区间的边界方向都有贡献，根据公式来确定该点对边界方向的贡献辐值，将相同方向中的点的辐值相加，辐值最大的那个方向确定为主方向；主方向为了确定旋转不变性；

2）根据主方向来构造，在每个特征点周围取$16*16$邻域，每$4*4$像素作为一个块，共16个块。每个块里面有8个值，共$16*8=128$个值，形成一个$1*128$的向量；用该向量作为描述向量；

SIFT可以调节的参数：

提取阶段：1.高斯金字塔下采样次数；2.对比度阈值

描述阶段：1.方向划分；2.领域大小（描述向量维度）；3.光照阈值；



2.3.DNN提取特征描述子

在三维重建中，DNN提取到的特征子结果并没有直接使用SIFT精确。



2.4.ICP（Iterated Closest Point）问题

不是一个特征子提取的方法，而是一个两个点集匹配的方法。

如果两个集合的点集中的点相互对应的上，则可以通过求解能量函数来得到最优解。没有匹配关系，可以先用粗糙方法估计一个匹配，然后使用迭代的方法来优化匹配。



2.5.鲁棒估计：RNASAC

可以用来做匹配。





### 第三章、图像分割

#### 1.概述

目标检测：将目标用box检测出来

图像分割：将图像不同的部分检测出来，包括语义分割和实例分割

语义分割：不同语义上的物体给区分出来

实例检测：可以将同一语义中的不同实例分割出来



图像分割是图像处理进到图像分析的关键步骤，是目标表达的基础。介于底层视觉和高层视觉之间，属于中层视觉。



#### 2.图像分割方法

1.早期的图像分割方法：阈值法，区域生长法，分裂合并法，基于边缘的分割方法

阈值分割法的关键是如何选取合适的阈值。



2.基于特定理论的方法

（1）基于“均值移动”的图像分割方法

例如找到一系列点的密度最大的中心点，可以随机确定一个中心和半径，在该领域内找到质量中心，再以该中心和相同的半径作领域，再寻找新的质量中心，继续迭代。

均值移动：

非参数估计：用不同领域的样本数量来代表各个领域的密度，适用于样本和整体存在差异的情况；

参数估计：先假设样本分布的密度函数是一个混合高斯分布函数，适用于样本可以代表整体的情况；



均值移动实际上是非参数估计的方法，是一种核密度梯度的估计方法：

核函数：$P(x)=\frac{1}{n}\sum_{i=1}^nK(x-x_i)$；

梯度核函数：



具体算法流程：

1）计算$m(x)$



该方法来做图像分割的过程实际上就是对每个像素作均值漂移的过程，最终达到一个收敛点。达到相同收敛点的视为一类；



（2）Graph Cut（图割）

将图像用图的方式表示，像素表示顶点，定点之间边权重表示像素之间的关系。图像分割实际上对应于图像的一个点割。

分割使得类间容量尽量大，类内容量尽量小。



2.基于深度网络的方法

（1）全卷积网络（CVPR2015）

全卷积网络，输入是图像，输出逐点的分类。

图像分类任务，前面是卷积和池化层，后面几层是全连接层，用于分类；

图像分割任务，所有的都是卷积池化层，最后几层使用上采样恢复原始图像大小；

（上采样（upsampling）：增大图像尺寸，主要方法有反卷积；）

只用最后一层做预测会出现偏差，引入Skip结构，多使用几层中间的信息。



（2）SegNet（PAMI2017）

编码解码结构，基本对称。

使用了不一样的一种上采样的方法：在池化的过程，记录最大池化元素的索引，为了后续上采样的时候使用。在同样尺寸的层，将一个元素上采样成四个元素。



（3）DeepLab（v1，v2，v3，v3+）

v1：提出空洞卷积，即卷积是有间隔的卷积，可以扩大感受野。





### 第四章、三维计算机视觉

#### 一、相机模型与多视几何

1.几何视觉与物体视觉

大脑背部通道进行空间视觉，目前主要采用基于几何的方法；

大脑腹部捅刀进行物体视觉，目前主要采用基于学习的方法。



2.计算机视觉框架

（1）Marr计算机视觉框架：图像-特征提取-2.5维深度图-三维模型-理解

（2）Poggio计算机视觉框架：图像-特征提取-子空间学习-理解，人在识别的时候，并不需要还原一个三维模型

（3）Hinton计算机视觉框架：图像-深度学习-理解



如今的三维计算机视觉任务很多算法中，仍然使用Marr的框架，因为还原三维模型对一般的任务有很多帮助，少量任务也会使用深度学习框架。



3.三维计算机视觉应用

（1）三维数字城市：输入是大量二维图形，输出一个三维的数字模型；

（2）运动捕捉：娱乐方面，在演员的脸上贴上红外标记点，重建出整个三维模型；

（3）无人驾驶



4.三维计算机视觉的核心研究内容

（1）场景结构

（2）相机位姿：六自由度位姿（$x,y,z$和相机朝向）



5.研究方法

（1）SfM：Structure from Motion，输入是多视角图像，重建场景稀疏结构与相机位姿。具体流程为：先离线重建三维结构，再在线计算其他的相机位姿

（2）SLAM：Simultaneous Localization and Mapping，输入视频序列，重建结构和相机位姿。

有时候三维结构不会发生变化，直接选择SfM即可，直接离线重建三维结构即可。



6.小孔成像

（1）小孔实际上是光圈，尽量采用小光圈，曝光时间长，得到精确图像；大光圈曝光时间短，得到模糊图像；光圈过小，会发生衍射。

（2）现代相机系统，用的是小孔成像模型，但是不是真正的小孔，而是通过透镜系统完成，使得在曝光时间短的情况下，还能得到一个倒立清晰的像。

（3）透镜系统中，不是所有范围的物体都可以清晰成像，由于人眼对模糊图像会有个容忍程度，所以在真实的成像系统中，聚焦平面在前后两个极限位置中间都可以视为是清晰成像，两个极限位置称为弥散圈，弥散圈中间的距离称为景深。光圈越大，景深越小，光圈越小，景深越大。

（4）单反相机获取的是真实的光线，而是手机的拍摄过程，手机上的结果是已经经过数模转换之后的图像。



7.三维物体小孔成像的数学表达

（1）小孔成像将三维坐标系原点定义在相机光心上，z轴垂直成像平面。欧式空间中，三维坐标会出现无穷大的情况，对计算造成影响，需要给出射影空间的定义。

（2）射影空间：对$n$维欧式空间加入无穷远元素，并对有限元素和无穷远元素不加区分，则他们共同构成$n$维射影空间，记作$P^n$。

一条直线只有唯一一个无穷远点，两个端点是为一个点；

一个平面，所有无穷远点组成一条直线，把边界想象成一个半径为无穷大的圆；

三维空间中所有无穷远点组成一个平面，把边界想象成一个半径为无穷大的球；

（3）齐次坐标：二维欧氏空间点$(x,y)$，转换到二维射影空间中为$(x,y,1)$，实际上就是非齐次坐标化为齐次坐标，$n$维欧氏坐标有$n$个元素，$n$位射影坐标有$n+1$个元素。

二维射影空间的点$(x,y,w)$，转换为二维欧氏空间中为$(\frac{x}{w},\frac{y}{w})$，实际上就是齐次坐标到非齐次坐标的转换。

（4）齐次坐标在相差一个尺度是等价，即$\lambda(x,y,z,1)=(x,y,z,1)$，否则会对应到多个欧氏空间点；无穷远点的齐次坐标为$(x,y,z,0)$，转换为非齐次坐标的时候，任何一个元素除以0都是无穷大，可以表示无穷远点。

（5）计算的时候，$f$的单位是像素，而不是物理单位

（6）以上定义的相机坐标，会随着相机移动而变化，需要把坐标定义在世界坐标系中，进而得到图像平面的射影坐标和世界坐标系的射影坐标之间的对应关系。



8.多视几何

（1）通过单幅图像无法重建场景结构，因为成像过程是一个降维的过程，失去了深度信息。只要有两副成像结果即可获得三维重建结果。

（2）两视图几何





#### 二、相机标定与稀疏重建

1.简介

通过大量的二维图像信息，重建出图像中包含的三维信息以及相机的空间位姿。有以下几种情况：

（1）三角化问题：

1）已知$x,K,R,t$，求解$X$。最小化重投影误差平方和，求解方法通常称为DLT（Direct Linear Transformation），此时的解称为代数误差解（可以解析地进行求解，比较容易得到，没有几何意义）；

2）真正场景的求解一般将代数误差解作为初值，使用迭代算法迭代求解几何误差解。



（2）相机标定：

1）已知$x,X$，求解$K,R,t$。主要目的要获得相机位置$R,t$和相机内参数$K$，一组2D-3D对应点提供关于$P$的两个线性方程，只需六组2D-3D对应点即可求解矩阵$P$，并且可以将列向量最后一个参数$p_{34}$归一化为1，只求解前11个参数；

2）求解出矩阵$P$之后，需要从矩阵$P$中分解出相机内外参数$K,R,t$；使用$QR$分解，得到上三角阵和正交矩阵；

以上属于三维标定法

3）平面标定板：求解单应$H$，只需要四组对应点

4）消影点标定方法

不管是哪种相机标定方法，都采用的DLT求解方法，优点是线性求解较简单，缺点是求解出来的是代数误差解，无几何意义，并且不包含相机畸变参数，无法添加其他约束条件。



（3）姿态估计：

1）已知$x,X,K$，求解$R,t$，还是使用DLT方法求解，6组2D-3D对应点肯定是求解$P$矩阵的，但不是最小配置解，因为此时$K$已知，相比于相机标定多了已知条件。此时最少需要3组2D-3D对应点求解。

2）求解主要使用PnP(Perspective-n-Point)算法



（4）稀疏重建：

1）已知$x$，求$K,R,t,X$，



**以上四种问题，求解流程都是一样的：先通过线性方法求解初始值（代数误差最小化），再通过非线性优化迭代求解（几何误差最小化）。**下面介绍非线性迭代算法：

重投影误差最小化是一个非线性最小二乘问题，所以一般求解非线性最小二乘问题的方法都可以求解重投影误差问题；而非线性最小二乘问题又是非线性优化问题的一种特殊形式，所以一般求解非线性优化问题的方法也可以求解非线性最小二乘问题，更可以求解重投影误差问题。

针对非线性优化问题，有梯度下降法，牛顿法；

针对非线性最小二乘问题，有高斯牛顿法，LM法；



目标函数关于自变量求导：（1）目标函数是标量，自变量是标量，求导称为导数；（2）目标函数是标量，自变量是向量，求导称为梯度，二阶导称为Hesse阵；（3）目标函数是向量，自变量是向量，求导称为Jacobi矩阵。

1.梯度下降法

梯度下降法实质上是在迭代点处对目标函数进行一阶泰勒展开，用一阶项去近似目标函数寻找梯度方向。所以当目标函数的高阶成分比较多时，每次迭代的方向都不能代表目标函数的真实最优方向，所以梯度下降法非常地缓慢。（由此提出二阶梯度方向的迭代算法-牛顿法）



2.牛顿法

牛顿法实质上是在迭代点处对目标函数进行二阶泰勒展开，所以更加接近目标函数的真实最优方向，但是每次的计算量比较大（需要计算Hesse阵才能得到每次的迭代方向）。



3.高斯牛顿法

介于梯度下降法和牛顿法之间，速度接近牛顿法，并且每次不用计算Hesse阵。缺点是每次迭代的方向可能是错误的方向，所以提出阻尼高斯牛顿法。



4.阻尼高斯牛顿法

其中LM方法一种启发式的阻尼高斯牛顿法，可以动态地调整$\lambda$的值，使得迭代过程介于梯度迭代法和高斯牛顿法之间。



通过点对点的方式，来重建三维物体中的点，这样的重建结果实际上很稀疏，下一节介绍解决方法。





#### 三、立体视觉与三维建模

1.通过输入的大量图像数据，相机的位姿、内参数以及稀疏的点云，重建出完整精确的三维结构。

2.情形：室内环境（光照可控、相机稳定），室内环境（自然光线，手持拍摄），网络图片（不可控）。

3.图像一致性：三维结构的不同视图的二维图像应该满足一定的相似性。

4.视差：人的两只眼睛看同一物体，两只眼睛的成像有一个水平距离，称为视差。物体离的越远，视差越小；离得越近，视差越大。







### 第五章、运动分析

定义：在不需要人为干预的情况下，综合利用模式识别，计算机视觉等技术，达到行为理解的目的；



### 一、运动检测

1.定义：将运动前景从图像序列中提取出来，将前景和背景分开。是后续目标跟踪、运动表述和行为理解等的基础；

2.两种思路：（1）直接得到前景图像；（2）得到背景图像，用输入图像减去背景图像得到前景图像。

3.背景差法（适用于背景相对稳定的视频场景）

（1）原理：计算当前图像和背景图像的逐像素的灰度图，再通过设置阈值来确定运动前景区域；

（2）均值图像：在已知背景的情况下，获取均值图像；

均值图像：将若干背景图像求逐点的灰度均值，因为不同帧的光照条件可能发生变化，对于同一个场景也会存在灰度的差异，通过取均值来消除差异。

（3）方法一，单高斯模型：对于每一个像素，用一个高斯分布来描述其在不同时刻的灰度值；（每一个像素都有一个高斯分布）

单高斯模型不足：背景往往不是绝对静止的，而是时常变化的，尤其对于室外场景（如树枝摇曳、窗帘晃动），提出混合高斯模型。

（4）方法二，混合高斯模型：对于每个像素，用一个混合高斯分布来描述其在不同时刻的灰度值；（每一个像素对于一个混合高斯分布）

1）数学原理：任何一个连续分布都可以用充分多的高斯分布来加权近似表示；

2）解释：针对图像中一个固定像素，数据来源是所有帧的该像素位置的灰度值，不同像素的混合高斯模型互不影响；

3）步骤：以某个确定的像素位置为例，第一帧的值先确定一个模型，第二帧和后续的再继续帧的流程按照流程图进行；



4.光流法

（1）运动场和光流场：物体运动的表征和物体灰度变换的表征；

（2）研究光流的原因：仅仅通过图像序列很难计算出物体的空间位置进而得到真实的运动场。而光流表达了图像的变化，包含了目标一定的运动性质；

（3）光流约束方程（光流的一致性假设）：

光流表示图像的变化：$I(x,,y,t)=I(x+u\Delta t,y+v\Delta t,t+\Delta t)$，图像某像素的灰度值经过一个短暂的时间后灰度值不变，每次给定一个方程就有一个$u,v$的约束条件。

（4）光流的五种计算方法：（只介绍基于梯度的方法）

将求未知数问题，转换为优化问题，求导。

**光流实际上就是计算前后两帧的灰度图变化矢量图**，得到光流变化之后，计算每个像素点的光流值，大于一定的阈值就认为是前景图，反之是背景图。优点是不易受运动物体的表观影响，比较直观；缺点是计算误差大，噪声比较大。



5.帧间差分

计算帧之间的灰度差值，大于一定阈值认为是前景图，反之是背景图。



#### 二、目标检测

1.传统方法：Adaboost

根据已有的样本，融合多个弱分类器形成一个整体的强分类器，并提高分类准确率。

AdaBoost：每个样本都会被赋予一个权重，错分样本的权重会增大；每个分类器也有一个权重，准确率越高的分类器权重越大。

初始化各个样本的权重，先给定一个分类器来分类样本，得到分类误差，利用该误差来计算该分类器的权重，利用分类器的权重来更新样本权重；输入下一个分类器，计算分类误差再给出当前分类器权重，以此类推。

实例：人脸检测



2.深度学习方法：

分为单阶段（YOLO系列）和两阶段方法（R-CNN），单阶段方法速度快，但是精度差一点；两阶段方法精度高，稳定性好一点。

（1）R-CNN

1）确定约2K个建议区域（region proposal），将每一个备选区域都输入到一个卷积网络里面，来判断是否为某一目标；

2）具体步骤：将2K个建议区域进行变换，输入到一个统一的卷积神经网络之中来提取特征（直接使用这个卷积网络分类精度会低一些），再使用一个分类器（如SVM）来分类（需要训练多个SVM，每个SVM只分类两类，如是否为狗），在分类的同时再进行一个分类框的回归（IoU指标来衡量）；



（2）Fast R-CNN和Faster R-CNN也很类似

（3）YOLO:直接回归到检测框



### 第六章、表达与识别

1.表达

**运动表达：**

（1）运动轨迹；

（2）时空图表达：将图像序列的前景运动信息和时间信息用一张图表示出来；

运动能量图：通过帧间差分，得到前景的二值图像，将视频序列中所有帧的前景二值化图像求并集；不足：动作顺序相反的运动能量图是一样的；

运动历史图：通过帧间差分，得到前景的二值图像，每个像素点的值会反应一定的历史信息；

**基于DNN的表达：**

神经网络的特征表达，每一层的输出都可以认为是输入图像的某种表达。

**稀疏、低秩表达：**

（1）在一定条件下，0-范数问题有最优解；

（2）在RIP条件下，0-范数问题等同于1-范数问题；

（3）1-范数问题是个凸优化问题；

稀疏表达约束的是表达系数的非零元素个数尽可能少，低秩表达约束的是表达系数的秩尽可能小。



2.行为识别

（1）基于模版匹配的方法：时间模版和动态时间归整；

（2）

（3）基于深度学习的方法：









#################################################################

## 雷震《视频监控与应用》

内容：目标检测，人脸关键点定位，人脸识别（给定两张人脸图像，计算出两张图像的相似度分数），目标再识别，行为分析



### 第一章、简介

**智能技术**

低级智能：看清楚（图像去噪、去雾、防抖等预处理，图像增强还原）

中级智能：看明白（目标分类及跟踪、图像区域分割与拼接、非法入侵）

高级智能：自动化（人脸、车牌的自动记录和识别、行为的判断与识别）



**运动目标检测**

运动目标检测区别于图像的目标检测（静态目标检测），具体方法有：背景差法，帧间差法，光流法

1.背景差法

（1）背景图像

1）将若干背景图像的均值图像作为背景，而不是直接从中选一张作为背景，因为均值滤波相当于低通滤波，可以滤掉高频信息，保留稳定的信息。

2）判断像素为背景像素的若干假设：

背景在图像序列中总是最经常被观测到；

像素点处于稳定状态最长的灰度值是背景像素灰度值；

3）单高斯模型来判断多个帧的背景图像：不够鲁棒

4）混合高斯模型判断多个帧的背景图像：

（2）缺点：当物体之间有阻挡的时候，如一辆车被一棵树挡住，目标检测可能会将车检测为两段



2.帧间差分

（1）相邻两帧间计算逐像素差

（2）优点：算法简单、易于实现、速度快

（3）缺点：噪声较多，检测精度不高



**运动目标分类**





### 第二章、目标检测

1.定义

目标检测，判断一副图像上是否存在感兴趣的物体，如果存在，就给出**所有**感兴趣物体的类别和位置（what and where）；



2.应用

如人数统计，通过检测人脸来统计数量；交通监控等。检测是各种产品中的第一步，位于各个算法之前；检测精度的高与低，影响着后续步骤的性能；



3.传统方法

（1）利用手工特征和分类器，使用滑窗操作，遍历所有位置

人脸有大有小，为了检测出所有人脸，需要改变框的大小来遍历。具体方法为：对图像做金字塔缩放，使用一个固定大小的框来遍历所有图像检测出人脸。对每个框中的patch，提取手工特征（如LBP,HOG,HAAR,SIFT等），来训练一个分类器（如adaboost,SVM,随机森林）



4.深度学习方法

（1）两阶段方法，R-CNN

得到若干候选框，对每个候选框都进行预测，非常耗时；

（2）两阶段方法，Fast R-CNN

候选区域太多有重叠，浪费时间，Fast R-CNN直接将整张图输入CNN，输出一个feature map，在该feature map上选取各种proposal；缺点：生成候选区域非常满；

（3）两阶段方法，Faster R-CNN

使用一个RPN代替候选区域的生成，同时RPN和CNN共享了卷积层，进一步减少了计算量；

（4）一阶段方法，YOLO，SSD



指标：

准确率：检测出来的，有多少是需要的目标；

召回率：需要的目标能检测出来多少，如100个目标，检测出来90个，则召回率为90%；



5.基准数据集

Pascal VOC，MS COCO等



6.深度学习训练技巧

（1）数据增强：增加深度模型鲁棒性和泛化性的常用手段，通过对数据变换，为了增加模型的一般不变性（如旋转不变性和平移不变性）；



7.目标检测未来方向

（1）解决检测中分类和回归的矛盾

分类：较深层的抽象特征，具备位置不变性，有利于分类；

回归：较浅层的细节特征，具备位置敏感性，有利于回归；

解决方法：高低层特征融合，先用卷积卷小特征，再用反卷积卷大特征，最后融合对应层特征，使得各层特征即抽象又细节丰富。

（2）使用轻量级的网络作为backbone来减速检测

（3）Anchor-free算法

之前的方法都是Anchor-based的，先要铺设Anchor



### 第三章、视频压缩



### 第四章、人脸关键点检测

1.定义：人脸关键点检测：定位人脸上一些固有语义的点，如眼角、鼻尖、嘴角

2.应用：分为稀疏关键点（用于人脸识别等）和稠密关键点（用于人脸增强现实）

3.深度学习提升的方法：加入分类失败的数据再训练



